services:
  # Zookeeper - necessário para o Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - flink-network

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - flink-network

  # Kafka UI (opcional, mas muito útil)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - flink-network

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - flink-network

  # DynamoDB Local
  dynamodb:
    image: amazon/dynamodb-local:latest
    container_name: dynamodb-local
    command: "-jar DynamoDBLocal.jar -sharedDb -inMemory"
    ports:
      - "8000:8000"
    networks:
      - flink-network

  # DynamoDB Admin (Interface Web para DynamoDB)
  dynamodb-admin:
    image: aaronshaf/dynamodb-admin:latest
    container_name: dynamodb-admin
    depends_on:
      - dynamodb
    ports:
      - "8001:8001"
    environment:
      DYNAMO_ENDPOINT: http://dynamodb:8000
      AWS_REGION: us-east-1
      AWS_ACCESS_KEY_ID: local
      AWS_SECRET_ACCESS_KEY: local
    networks:
      - flink-network

  # Flink JobManager
  jobmanager:
    image: flink:1.18.0-scala_2.12-java11
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=local
      - AWS_SECRET_ACCESS_KEY=local
    volumes:
      - flink-checkpoints:/tmp/flink-checkpoints
      - flink-savepoints:/tmp/flink-savepoints
      - ./jobs:/opt/flink/usrlib
    networks:
      - flink-network

  # Flink TaskManager
  taskmanager:
    image: flink:1.18.0-scala_2.12-java11
    depends_on:
      - jobmanager
    command: taskmanager
    deploy:
      replicas: 2
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 4
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=local
      - AWS_SECRET_ACCESS_KEY=local
    volumes:
      - flink-checkpoints:/tmp/flink-checkpoints
      - ./jobs:/opt/flink/usrlib
    networks:
      - flink-network

  # Spark Master
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=1
    ports:
      - "7077:7077"
      - "8082:8080"
    volumes:
      - ./sparkjob:/opt/spark-apps
      - spark-checkpoints:/tmp/spark-checkpoints
    networks:
      - flink-network

  # Spark Worker
  spark-worker:
    image: apache/spark:3.5.0
    depends_on:
      - spark-master
    user: root
    command:
      - /bin/bash
      - -c
      - |
        pip install -q boto3 elasticsearch
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=1
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    deploy:
      replicas: 2
    volumes:
      - ./sparkjob:/opt/spark-apps
      - spark-checkpoints:/tmp/spark-checkpoints
    networks:
      - flink-network

  # Spark Job Runner
  spark-job:
    image: apache/spark:3.5.0
    container_name: spark-job
    user: root
    depends_on:
      - spark-master
      - kafka
      - dynamodb
      - elasticsearch
    environment:
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=local
      - AWS_SECRET_ACCESS_KEY=local
    volumes:
      - ./sparkjob:/opt/spark-apps
      - spark-checkpoints:/tmp/spark-checkpoints
    working_dir: /opt/spark-apps
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /tmp/spark-checkpoints
        chmod 777 /tmp/spark-checkpoints
        apt-get update -qq && apt-get install -y -qq zip python3 python3-pip > /dev/null 2>&1
        python3 -m pip install boto3 elasticsearch
        cd /opt/spark-apps/src
        zip -q ../modules.zip *.py
        cd /opt/spark-apps
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --py-files /opt/spark-apps/modules.zip \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.0 \
          --conf spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoints \
          --conf spark.sql.shuffle.partitions=10 \
          --conf spark.pyspark.python=/usr/bin/python3 \
          --conf spark.pyspark.driver.python=/usr/bin/python3 \
          --conf spark.executorEnv.PIP_PACKAGES="boto3 elasticsearch" \
          /opt/spark-apps/src/receivable_aggregator.py
    networks:
      - flink-network
    profiles:
      - spark-streaming

networks:
  flink-network:
    driver: bridge

volumes:
  elasticsearch-data:
    driver: local
  dynamodb-data:
    driver: local
  flink-checkpoints:
    driver: local
  flink-savepoints:
    driver: local
  spark-checkpoints:
    driver: local
